# Aldi Scraper vers JSON statique (GitHub Pages)

- G√©n√®re `data/products.json` (complet) et `data/products-min.json` (minimal) √† partir des indices Algolia d'ALDI Belgique.
- Pipeline automatis√© via GitHub Actions (hebdomadaire), commits uniquement si modifications.

## Setup local
- Pr√©requis: Python `3.10+`, `pip`, acc√®s r√©seau.
- Installer les d√©pendances: `python3 -m pip install -r requirements.txt`
- Configurer l‚Äôacc√®s Algolia:
  - Obligatoire: `export ALGOLIA_API_KEY="<votre_clef>"`
  - Optionnel (valeurs par d√©faut d√©j√† int√©gr√©es):
    - `export ALGOLIA_APP_ID="W297XVTVRZ"`
    - `export ASSORTMENT_INDEX="prod_be_fr_assortment"`
    - `export OFFERS_INDEX="prod_be_fr_offers"`
    - `export HITS_PER_PAGE=1000` (pour paginer plus ou moins)
    - `export GLOBAL_TIMEOUT_SECONDS=300`
    - `export PAGE_DELAY_MIN_MS=300` (d√©lai minimum entre requ√™tes, en ms)
    - `export PAGE_DELAY_MAX_MS=900` (d√©lai maximum entre requ√™tes, en ms)
    - `export MAX_PAGES_SAFETY_LIMIT=100` (limite de s√©curit√© anti-boucle infinie)
- Lancer en module: `python3 -m scripts.scraper`
- Alternative: `python scripts/scraper.py`

Astuce: vous pouvez cr√©er un fichier `.env` √† la racine; le scraper le charge automatiquement s'il existe. En production/CI, utilisez des variables d'environnement.

Utiliser `.env` (recommand√© en local):
- Copier l‚Äôexemple: `cp .env.example .env`
- √âditer `ALGOLIA_API_KEY=...` (et, si besoin, `ALGOLIA_APP_ID`, `ASSORTMENT_INDEX`, `OFFERS_INDEX`)
- Lancer ensuite `python3 -m scripts.scraper` (les valeurs de `.env` seront prises en compte)

## Tests
- `pytest -q`

## Secrets
- Dans GitHub ‚Üí Settings ‚Üí Actions ‚Üí Secrets: ajouter `ALGOLIA_API_KEY`.

## Fichiers g√©n√©r√©s
- `data/products.json` (complet, avec meta)
- `data/products-min.json` (essential fields)
- `data/metadata.json` (meta r√©sum√©)

## O√π trouver et comment lire les r√©sultats
- `data/products.json` (complet)
  - `meta.schema_version`: version du sch√©ma.
  - `meta.last_updated`: ISO datetime UTC de la g√©n√©ration.
  - `meta.total_products`: nombre total d‚Äôarticles fusionn√©s.
  - `meta.source`: source (`algolia`).
  - `meta.indices`: indices interrog√©s (`assortment`, `offers`).
  - `products`: tableau des documents bruts (champs Algolia tels que `objectID`, `productName`, `salesPrice`, `productPicture`, etc.).
- `data/products-min.json` (simplifi√©)
  - `products[]` contient des champs uniformis√©s pour l‚Äôusage courant:
    - `id`: identifiant (depuis `objectID`).
    - `name`: nom produit (depuis `productName`/`name`).
    - `price`: prix num√©rique si disponible (priorit√© √† `salesPrice`, fallback `priceFormatted`).
    - `category`: cat√©gorie heuristique bas√©e sur le nom (peut √™tre `autres`).
    - `image_url`: URL d‚Äôimage (priorit√© √† `productPicture` ou premier lien des renditions).
    - `is_promotion`: bool√©en, `true` si pr√©sent dans l‚Äôindex `offers`.
    - `promo_text`: texte promo/description, parfois HTML.
    - `valid_until`: date de fin si disponible, souvent `null` (peu expos√©e par Algolia).
    - `unit`: unit√© affich√©e (ex. `salesUnitFormatted`, `salesUnit2`).
- `data/metadata.json`: r√©sum√© minimal (`schema_version`, `last_updated`, `total_products`).

## Contr√¥les rapides
- Compter les produits: `jq '.products | length' data/products-min.json`
- Compter les promotions: `jq '[.products[] | select(.is_promotion)] | length' data/products-min.json`
- V√©rifier les champs manquants: `grep -c '"price": null' data/products-min.json` etc.

## Optimisations de pagination

Le scraper utilise une pagination optimis√©e avec les caract√©ristiques suivantes:

- **D√©lais entre requ√™tes**: 300-900ms al√©atoires pour simuler un comportement humain et √©viter le rate-limiting
- **Logging progressif**: affichage du num√©ro de page, hits par page, et total cumul√©
- **Gestion d'erreurs robuste**: retour des r√©sultats partiels en cas d'erreur, plut√¥t qu'√©chec total
- **Limite de s√©curit√©**: maximum 100 pages pour pr√©venir les boucles infinies

**Limitation importante**: L'API de recherche Algolia a une limite stricte de **1000 r√©sultats maximum** par requ√™te, m√™me avec pagination. Pour d√©passer cette limite, il faudrait soit:
- Utiliser l'API Browse (n√©cessite des permissions diff√©rentes)
- Effectuer plusieurs requ√™tes filtr√©es (par cat√©gorie, prix, etc.)

R√©sultats actuels: ~1270 produits (1000 de `assortment` + ~270 de `offers`)

## Interpr√©tation et limites
- `price`: certains articles n‚Äôont pas de prix dans la source, `null` est normal.
- `valid_until`: la date de fin de promotion n‚Äôest pas toujours fournie dans les hits; peut rester `null`.
- `category`: bas√©e sur des mots-cl√©s; pour une cat√©gorisation pr√©cise, mappez `hierarchicalCategories.lvl3/lvl4` vers vos cat√©gories.
- `promo_text`: peut contenir des balises HTML; rendez-le texte selon vos besoins.

## D√©pannage
- Erreur `403 Forbidden`:
  - V√©rifiez que `ALGOLIA_API_KEY` est une cl√© ‚Äúsearch-only‚Äù autoris√©e pour l‚Äô`ALGOLIA_APP_ID` et les indices.
  - Le scraper ajoute `Origin`, `Referer`, `User-Agent` et `x-algolia-agent` pour mimer un navigateur.
  - Test rapide via `curl`:
    - `curl -s -i -X POST "https://W297XVTVRZ-dsn.algolia.net/1/indexes/*/queries" \
      -H "X-Algolia-Application-Id: W297XVTVRZ" \
      -H "X-Algolia-API-Key: $ALGOLIA_API_KEY" \
      -H "Origin: https://www.aldi.be" -H "Referer: https://www.aldi.be/" \
      -H "User-Agent: Mozilla/5.0" \
      --data '{"requests":[{"indexName":"prod_be_fr_assortment","params":"hitsPerPage=1&page=0"}]}'`
- Erreur `Missing ALGOLIA_API_KEY environment variable.`: exportez correctement la cl√© avant de lancer.
- Mock pour diagnostic: `python3 -m scripts.scraper --dump-mock` (affiche un exemple de r√©ponse Algolia).

## CI/CD
- Workflow: `.github/workflows/scrape-aldi.yml` (ex√©cution hebdomadaire ou d√©clenchement manuel).
- Secrets requis: `ALGOLIA_API_KEY`.
- Les artefacts g√©n√©r√©s (fichiers `data/`) sont commit√©s automatiquement si le contenu a chang√©.

## GitHub Pages
- Activer Pages: `Settings ‚Üí Pages ‚Üí Build and deployment ‚Üí Deploy from a branch`.
- S√©lectionner `Branch: main` et `Folder: /(root)`.
- L‚ÄôURL sera `https://<username>.github.io/<repo>/`.
- V√©rifier vos JSON: `https://<username>.github.io/<repo>/data/products.json`.

## D√©clenchement manuel (Actions)
- Aller dans `Actions ‚Üí üõí Scrape ALDI Products`.
- Cliquer `Run workflow` pour lancer le scraping et la publication.

## Publier en une commande (local)
- Rendre le script ex√©cutable: `chmod +x scripts/publish.sh`
- Lancer: `./scripts/publish.sh`
- Le script:
  - Ex√©cute le scraper localement.
  - Commit et push les fichiers `data/*.json` si changements.
  - N√©cessite que `origin` pointe vers `https://github.com/<username>/<repo>.git` et que vous soyez authentifi√©.

### Authentification Git la plus simple
- Option recommand√©e: GitHub CLI
  - Installer: `brew install gh`
  - Se connecter: `gh auth login` (choisir GitHub.com, HTTPS, ouvrir le navigateur pour autoriser).
- Alternative: PAT (token personnel) pour `git push` HTTPS.

## Re-g√©n√©rer le minimal sans re-scraper
- Si `data/products.json` existe d√©j√†, vous pouvez r√©g√©n√©rer `products-min.json` en relan√ßant le scraper ou en ex√©cutant un petit script utilisant `AldiScraper().build_min(...)` sur les produits fusionn√©s.

## Personnalisation
- Ajuster `HITS_PER_PAGE` pour g√©rer le volume par page.
- Am√©liorer la cat√©gorisation en mappant `hierarchicalCategories` vers un ensemble de cat√©gories m√©tier.